<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>hx-nous</title><link>https://hx-nous.github.io</link><description>记录研究生学习历程</description><copyright>hx-nous</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://avatars.githubusercontent.com/u/119385115?v=4</url><title>avatar</title><link>https://hx-nous.github.io</link></image><lastBuildDate>Mon, 10 Nov 2025 03:26:16 +0000</lastBuildDate><managingEditor>hx-nous</managingEditor><ttl>60</ttl><webMaster>hx-nous</webMaster><item><title>Graph Foundation Models: A Comprehensive Survey</title><link>https://hx-nous.github.io/post/Graph%20Foundation%20Models-%20A%20Comprehensive%20Survey.html</link><description>整篇文章是对**图基础模型（Graph Foundation Models, GFMs）**的全面调查，旨在探索如何将大规模预训练的“基础模型”技术（类似于自然语言处理和计算机视觉领域中的大模型）应用于图结构数据。</description><guid isPermaLink="true">https://hx-nous.github.io/post/Graph%20Foundation%20Models-%20A%20Comprehensive%20Survey.html</guid><pubDate>Mon, 10 Nov 2025 03:25:50 +0000</pubDate></item><item><title>GraphGPT: Graph Instruction Tuning for Large Language Models</title><link>https://hx-nous.github.io/post/GraphGPT-%20Graph%20Instruction%20Tuning%20for%20Large%20Language%20Models.html</link><description>## 主要内容与核心方法

### 目标：
作者想把“图结构知识”真正注入到大语言模型（LLM）里，让模型在零样本与跨数据集/跨任务的图学习场景中也能泛化（如节点分类、链路预测），而不是只靠下游少量标注微调的传统GNN路线。</description><guid isPermaLink="true">https://hx-nous.github.io/post/GraphGPT-%20Graph%20Instruction%20Tuning%20for%20Large%20Language%20Models.html</guid><pubDate>Mon, 03 Nov 2025 12:41:56 +0000</pubDate></item><item><title>LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings</title><link>https://hx-nous.github.io/post/LLMs%20as%20Zero-shot%20Graph%20Learners-%20Alignment%20of%20GNN%20Representations%20with%20LLM%20Token%20Embeddings.html</link><description># 论文总结

## 一、研究背景
图机器学习中，图神经网络（GNN）泛化能力有限，跨数据集 / 任务时性能不稳定，且依赖大量标注数据；现有自监督学习（如 DGI）、图提示学习需任务特定微调，实用性受限。</description><guid isPermaLink="true">https://hx-nous.github.io/post/LLMs%20as%20Zero-shot%20Graph%20Learners-%20Alignment%20of%20GNN%20Representations%20with%20LLM%20Token%20Embeddings.html</guid><pubDate>Thu, 16 Oct 2025 08:42:42 +0000</pubDate></item><item><title>GL-Fusion: Rethinking the Combination of Graph Neural Network and Large Language model阅读</title><link>https://hx-nous.github.io/post/GL-Fusion-%20Rethinking%20the%20Combination%20of%20Graph%20Neural%20Network%20and%20Large%20Language%20model-yue-du.html</link><description>#  粗略阅读

## 本文主要介绍了一种GNN&amp;LLM结合的新架构，主要目的是避免以往的结合方式中出现的一些缺陷

### 创新点

1. 结构感知Transformer层：在 Transformer 层内嵌入 message-passing（节点级聚合与更新），并通过特殊 attention mask 保证文本的自回归因果性与图节点的置换不变性。</description><guid isPermaLink="true">https://hx-nous.github.io/post/GL-Fusion-%20Rethinking%20the%20Combination%20of%20Graph%20Neural%20Network%20and%20Large%20Language%20model-yue-du.html</guid><pubDate>Tue, 07 Oct 2025 13:05:18 +0000</pubDate></item><item><title>9.26-Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining</title><link>https://hx-nous.github.io/post/9.26-Large%20Language%20Models%20Meet%20Graph%20Neural%20Networks-%20A%20Perspective%20of%20Graph%20Mining.html</link><description># 本篇综述主要讲解了GNN和LLM在图挖掘领域的结合相关情况

## 首先，本文讲解了图挖掘的重要性，GNN擅长结构建模，LLM擅长语义理解，然后列出了GNN和LLM在各自领域的应用。</description><guid isPermaLink="true">https://hx-nous.github.io/post/9.26-Large%20Language%20Models%20Meet%20Graph%20Neural%20Networks-%20A%20Perspective%20of%20Graph%20Mining.html</guid><pubDate>Fri, 26 Sep 2025 08:15:06 +0000</pubDate></item><item><title>MD-GNN阅读</title><link>https://hx-nous.github.io/post/MD-GNN-yue-du.html</link><description># 9.15组会汇报本篇论文
本论文主要是讲了一个带有多种自约束的多通道解耦图神经网络，目的是为了解决在使用图神经网络过程中的标签不足问题，相较于以往的多通道图神经网络，它有三个通道：特征通道，拓扑通道，潜在通道。</description><guid isPermaLink="true">https://hx-nous.github.io/post/MD-GNN-yue-du.html</guid><pubDate>Fri, 19 Sep 2025 08:18:15 +0000</pubDate></item></channel></rss>