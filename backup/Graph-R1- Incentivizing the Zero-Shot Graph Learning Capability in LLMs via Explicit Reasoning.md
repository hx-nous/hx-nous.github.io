论文要解决的问题：
现在主流的GNN与LLM结合的方法有以下几种：
１、将图扁平化为文本序列，并输入 LLM；然而，由于缺乏结构 性归纳偏置，这往往导致效果不佳
２、一种工作线保留 GNN 作为预测变量，同时使用 LLM 生成辅助信号，如合成标签或节点描述； 然而，这些方法仍然依赖于刚性 GNN 头，并且每项任务都需要重新训练
３、将预测工作委托给 LLM，同时通过跨模态投影 纳入来自冻结 GNN 的结构信号；但是，训练在各个组成部分间的分离导致任务条件作用薄弱，迁移性有限
４、在推理时直接将 GNN 特征注入 LLM 令牌流。虽然这提高了零射击精度，但会带来大量计算开销，并且在任务和领域间的泛化仍然存在困难

论文方法：
这篇文章主要是讲了只用LLM不用GNN来处理零样本图学习问题，主要是通过两个阶段来进行训练，首先自己创建了一个带推理的多领域、多任务的数据集，第一阶段：用这个数据集对DeepSeek-R1-Distill-Qwen-14B进行全参数微调；第二阶段：用基于Group Relative Policy Optimization (GRPO)的强化学习来对第一阶段得到的模型进行rethinking微调；最后结果表明取得了很好的效果。

创新点

不足

总结

笔记：
为什么要用 GRPO？
传统的强化学习（如 PPO）需要一个“Critic 模型”来打分，这非常占显存，而且在处理长文本推理时很难训练。
GRPO 的做法：它不需要 Critic 模型。它让模型对同一个问题生成一组（比如 8 个）不同的回答。
竞争机制：它计算这 8 个回答的平均得分。谁的得分比平均分高，就奖励谁；谁比平均分低，就惩罚谁。这就像“组内末位淘汰”，逼着模型不断探索更好的解题路径。

了解如何利用"推理"作为信号来指导适配

相比之下，GNN内部的高维表示和传播过程对人类几乎是不可解释的，通常只能借助事后分析（例如注意力可视化）略窥一二。Graph-R1则把理由写在明面上，方便用户理解和验证模型的结论

在语言模型中，极大似然训练指的是最大化训练语料中给定输入序列后目标输出序列的概率。
